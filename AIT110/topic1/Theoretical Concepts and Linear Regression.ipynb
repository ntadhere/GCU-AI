{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9791951",
   "metadata": {},
   "source": [
    "# Theoretical Concepts and Linear Regression\n",
    "\n",
    "## Part 1: Theoretical Concepts\n",
    "1. Define the following terms in the context of statistical learning:\n",
    "- **Training error**: It is the error the model makes on the trainning dataset. It measures how well a model performs on the training dataset\n",
    "- **Test error**: it evaluates the model's performance on unseen data. A low testing error indicates good generalization, while a high testing error may suggest overfitting or underfitting\n",
    "- **Bias-cariance trade-off**: describes the trade-off between a model's ability to fit the training data well (low bias) and its ability to generalize to unseen data (low variance)\n",
    "- **Overfitting**: When a model learns the training data too well, including the noise, and performs poorly on unseen data\n",
    "- **Model complexity**: how well a model can fit the training data and potentially generalize to new, unseen data\n",
    "\n",
    "2.  Explain the difference between parametric and nonparametric models in statistical learning. Provide an example of each type of model. \n",
    "\n",
    "| Parametric Models | Nonparametric Models |\n",
    "|:-----------|:------------|\n",
    "| fixed structure | Flexible |\n",
    "| Few parameters | Data-driven, Parameters grow with data |\n",
    "| simpler and computationally efficient | more computationally demanding and prone to overfitting with small datasets |\n",
    "| Linear regression | k-Nearest Neighbors |\n",
    "| Suppose we model house price based on square footage. A parametric model (linear regression) assumes a straight-line relationship: as square footage increases, price increases at a fixed rate. It simplifies reality into slope + intercept. | Instead of assuming a straight line, k-NN looks at prices of the most similar houses (neighbors in size, location, number of rooms, etc.) and predicts based on those. This adapts to neighborhoods where price jumps arenâ€™t linear (e.g., houses near a beach are disproportionately expensive). |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3.    Discuss the bias-variance trade-off in relation to model performance. Provide an example of how a highly complex model could lead to overfitting and how a simple model might underfit the data. \n",
    "- The bias-variance trade-off explains how model complexity affects prediction:\n",
    "    - **Simple models** have high bias (too rigid), which can cause *underfitting*-they miss important patterns\n",
    "    - **Complex models** have high variance (too sensitive to data), which can cause *overfitting*-they memorize noise instead of generalizing\n",
    "- Example:\n",
    "    - A simple linear model predicting house prices from size alone may underfit\n",
    "    - A deep neural network with many layers may overfit by memorizing training data\n",
    "- The goal is to balance bias and variance for the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6e69c",
   "metadata": {},
   "source": [
    "## References:\n",
    "- Lecture note: https://colab.research.google.com/drive/15Lq4kiSe4JkdOJifvjS4Ac2_jMODrdoI#scrollTo=pBHLavikDQUC\n",
    "- Model Complexity: https://ishanjainoffical.medium.com/model-complexity-explained-intuitively-e179e38866b6"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
